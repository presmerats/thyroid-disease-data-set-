MVA - thyroid0387
==================

We possibly want to combine PCA and MCA in our project. We were told to do the following:
1- normalize/arrange the principal components from each analysis in order to have 100% variance explained in total
2- combine them and select the sginificant components
3- project categorical vars to PCA significant principal components and project continouous vars to MCA singificant principal components


preprocessing

	ok-decision over TBG imputation
		-decide about TBG, T3 and others that have a lot of missing values
		if TSH abnormal T3,


	ok- sharelatex
	ok- bitbucket
	ok- main.R with other files for each subtask

	ok- categorical and continuous
			-> all continuous

	ok-summary
	ok-errors
	ok-missing values -> mice
	ok-outliers

	
	ok-	feature extraction (target 20 classes to 7 classes)
		- 7 classe only
		- 4 classes Hyper, Hypo, other, normal
		- 2 classe only





	unsupervised analysis 
		PCA?
			ok-with all vars
				-> preprocessing/missing/outliers/errors/binarization
			ok- feature selection
				ok-compute the contribution to the total inertia of each var
				ok-select PC's
					-> 17 by 90%
					-> 7 by last elbow: we choose this one
				ok-well represented vars
				ok-well represented modalities of the target
				-biplot? makes sense?

			ok- save PC's to df for later clustering

->		rotation


->		clustering
			Hierarchical + Kmeans (consolidation)

		visualization
			PCA, 
			PCA rot
			hierarchicacl clustering

		interpreation
			- PCA conclusions 
					var contribs : comparison with RF feature selection
						expectations:
							-the more important vars for prediction 
								-well represented in the first fact plane
								-that contributes the most to the total intertia in the first fact plane
							-well represented classes(modalities of target) -> will be better predicted
							-
					correlated vars
					correlated with target
					biplot?
			- profiling? -> catdes

			- for prediction
				decision trees: 
					needs balanced classes
					needs few important vars, not all
				random forest
					better if all vars are important
					does not need balanced classes


	ok-feature selection
->	- verify the contribution with PCA?


		prediction model
			ok-separate test and training?
			decision trees
				CART intended for balanced classes in the training data
			random forest
			validation protocol OOB
			harmonic measure -> macro-F measure
			compare dec. vs rf


Report:

obs:
	we expect rf to be better if a lot of vars are explinatory
	CART to be better if only a small subset of vars are explanatory

1. A description of the problem and available data

2. The pre-process of data


3. The protocol of validation
	-> about cv and oob for trees

4. The visualisation performed
	PCA + rotation	

5. The interpretation of the latent concepts.
	rotation


6. The clustering performed
7. The interpretation of the found clusters.
8. Discussion about the differences of the test sample respect to the
training one.
9. The prediction model with its best parameterization and its
generalization error
10.Scientific and personal conclusions